{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVLWhMHBrxc3"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7InO_Tv7rxc6"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import os\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XglUMLCrxdD"
      },
      "source": [
        "## 1. Crawl data from llama-index blogs and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTDRQ8bArxdE"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pickle\n",
        "import os\n",
        "import faiss\n",
        "from openai import OpenAI\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uaxicRqrxdE",
        "outputId": "aa0e91db-3336-4a89-a499-2ae4dcb2dcd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status Code: 200\n",
            "Page Title: Blog — LlamaIndex, Data Framework for LLM Applications\n"
          ]
        }
      ],
      "source": [
        "# Set up headers to mimic a browser request\n",
        "headers = {\n",
        "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "}\n",
        "\n",
        "# Making a GET request with headers\n",
        "try:\n",
        "    r = requests.get('https://www.llamaindex.ai/blog', headers=headers, timeout=10)\n",
        "    r.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "    # Print status code\n",
        "    print(f\"Status Code: {r.status_code}\")\n",
        "\n",
        "    # Parsing the HTML\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "    # Print the title of the page\n",
        "    print(f\"Page Title: {soup.title.string if soup.title else 'No title found'}\")\n",
        "\n",
        "except requests.RequestException as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFWwhNchrxdF",
        "outputId": "f4ce7a4e-a862-4305-fcc5-108f266cdffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "                    Title: One-click Open Source RAG Observability with Langfuse\n",
            "                    -----------\n",
            "                    Session title: None\n",
            "                    -----------\n",
            "                    Content: This is a guest post from the team at Langfuse\n",
            "There are so many different ways to make RAG work for a use case. What vector store to use? What retrieval strategy to use? LlamaIndex makes it easy to try many of them without having to deal with the complexity of integrations, prompts and memory all at once.\n",
            "Initially, we at Langfuse worked on complex RAG/agent applications and quickly realized that there is a new need for observability and experimentation to tweak and iterate on the details. In the end, these details matter to get from something cool to an actually reliable RAG application that is safe for users and customers. Think of this: if there is a user session of interest in your production RAG application, how can you quickly see whether the retrieved context for that session was actually relevant or the LLM response was on point?\n",
            "Thus, we started working on Langfuse.com (GitHub) to establish an open source LLM engineering platform with tightly integrated features for tracing, prompt management, and evaluation. In the beginning we just solved our own and our friends’ problems. Today we are at over 1000 projects which rely on Langfuse, and 2.3k stars on GitHub. You can either self-host Langfuse or use the cloud instance maintained by us.\n",
            "We are thrilled to announce our new integration with LlamaIndex today. This feature was highly requested by our community and aligns with our project's focus on native integration with major application frameworks. Thank you to everyone who contributed and tested it during the beta phase!    \n",
            "                    \n"
          ]
        }
      ],
      "source": [
        "def extract_blog_content(url):\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "\n",
        "    r = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "    # Parsing the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "    # Extract the blog title\n",
        "    blog_title = soup.find('h1')\n",
        "    if blog_title:\n",
        "        blog_title = blog_title.text.strip()\n",
        "    else:\n",
        "        blog_title = \"No title found\"\n",
        "\n",
        "    # Extract the blog content\n",
        "    content_div = soup.find('div', class_='BlogPost_htmlPost__Z5oDL')\n",
        "\n",
        "    chunks = []\n",
        "\n",
        "    count_session = 0\n",
        "\n",
        "    if content_div.find(['h2']) == None:\n",
        "        session_type = 'h3'\n",
        "        content_type = ['h3','p','blockquote','li','pre','figcaption']\n",
        "    else:\n",
        "        session_type = 'h2'\n",
        "        content_type = ['h2','h3','p','blockquote','li','pre','figcaption']\n",
        "\n",
        "    if content_div:\n",
        "        current_chunk = None\n",
        "        paragraphs = content_div.find_all(content_type)\n",
        "\n",
        "        for paragraph in paragraphs:\n",
        "            if paragraph.name == session_type:\n",
        "                if current_chunk:\n",
        "                    final_chunk = f\"\"\"\n",
        "                    Title: {current_chunk['title']}\n",
        "                    -----------\n",
        "                    Session title: {current_chunk['session_title']}\n",
        "                    -----------\n",
        "                    Content: {current_chunk['content']}\n",
        "                    \"\"\"\n",
        "                    chunks.append(final_chunk)\n",
        "\n",
        "                current_chunk = {\n",
        "                    'title': blog_title,\n",
        "                    'session_title': paragraph.text.strip(),\n",
        "                    'content': ''\n",
        "                }\n",
        "                count_session+=1\n",
        "            elif count_session > 0:\n",
        "                # Clean and add text of each paragraph to the current chunk content\n",
        "                text = paragraph.text.strip()\n",
        "                if text:  # Make sure not to add empty strings\n",
        "                    current_chunk['content'] += '\\n' + text\n",
        "            else:\n",
        "                text = paragraph.text.strip()\n",
        "                if current_chunk:\n",
        "                    if text:\n",
        "                        current_chunk['content'] += '\\n' + text\n",
        "                else:\n",
        "                    current_chunk = {\n",
        "                        'title': blog_title,\n",
        "                        'session_title': None,\n",
        "                        'content': text\n",
        "                    }\n",
        "\n",
        "        if current_chunk:\n",
        "            final_chunk = f\"\"\"\n",
        "            Title: {current_chunk['title']}\n",
        "            -----------\n",
        "            Session title: {current_chunk['session_title']}\n",
        "            -----------\n",
        "            Content: {current_chunk['content']}\n",
        "            \"\"\"\n",
        "            chunks.append(final_chunk)\n",
        "\n",
        "    return chunks\n",
        "# # Example usage\n",
        "url = 'https://www.llamaindex.ai/blog/one-click-open-source-rag-observability-with-langfuse'\n",
        "blog_chunks = extract_blog_content(url)\n",
        "for chunk in blog_chunks:\n",
        "    print(chunk)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTFAycEKrxdH",
        "outputId": "d7877b74-ba5f-4d1c-dc94-faf0807ada22"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 157/157 [00:59<00:00,  2.66it/s]\n"
          ]
        }
      ],
      "source": [
        "# Assuming the HTML content is stored in a variable called 'html_content'\n",
        "# soup = BeautifulSoup(soup, 'html.parser')\n",
        "\n",
        "# Find all blog post cards\n",
        "r = requests.get('https://www.llamaindex.ai/blog')\n",
        "soup = BeautifulSoup(r.content, 'html.parser')\n",
        "\n",
        "blog_cards = soup.find_all('div', class_='CardBlog_card__mm0Zw')\n",
        "base_url = \"https://www.llamaindex.ai\"\n",
        "\n",
        "# List of crawl content\n",
        "crawl_content = []\n",
        "\n",
        "# Extract and print the main text from each card\n",
        "for card in tqdm(blog_cards):\n",
        "    # Extract title\n",
        "    title_element = card.find('p', class_='CardBlog_title__qC51U').find('a')\n",
        "    url = base_url + title_element['href']\n",
        "\n",
        "    blog_chunks = extract_blog_content(url)\n",
        "\n",
        "    for chunk in blog_chunks:\n",
        "        crawl_content.append(chunk)\n",
        "\n",
        "if not(os.path.exists('crawl_content.pkl')):\n",
        "  # store the crawl content dictionary to a file\n",
        "  with open('crawl_content.pkl', 'wb') as crawl_file:\n",
        "      pickle.dump(crawl_content, crawl_file)\n",
        "else:\n",
        "  with open('crawl_content.pkl', 'rb') as crawl_file:\n",
        "      crawl_content = pickle.load(crawl_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WbBpP4FrxdI"
      },
      "outputs": [],
      "source": [
        "with open('../API_key', 'r') as f:\n",
        "    os.environ['OPENAI_API_KEY'] = f.read()\n",
        "\n",
        "# Initialize an OpenAI instance\n",
        "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "def get_text_embeddings(input):\n",
        "    embeddings_batch_response = client.embeddings.create(\n",
        "        model='text-embedding-3-small',\n",
        "        input = input\n",
        "    )\n",
        "    return embeddings_batch_response.data[0].embedding\n",
        "\n",
        "if not(os.path.exists('embeddings.pkl')):\n",
        "  text_embeddings = []\n",
        "  for content in tqdm(crawl_content):\n",
        "      content_embedding = get_text_embeddings(content)\n",
        "      text_embeddings.append(content_embedding)\n",
        "  text_embeddings = np.array(text_embeddings)\n",
        "\n",
        "  # Save embeddings to a file\n",
        "  with open('embeddings.pkl', 'wb') as f:\n",
        "      pickle.dump(text_embeddings, f)\n",
        "else:\n",
        "  with open('embeddings.pkl', 'rb') as f:\n",
        "      text_embeddings = pickle.load(f)\n",
        "\n",
        "print('Shape of text embedding: ',text_embeddings.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc4qUuYQrxdK"
      },
      "outputs": [],
      "source": [
        "d = text_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(text_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdkUWkfIrxdL",
        "outputId": "48608839-1fea-40e8-abe2-82811b3bff5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question embedding shape:  (1, 1536)\n"
          ]
        }
      ],
      "source": [
        "question = 'What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?'\n",
        "question_embeddings = np.array([get_text_embeddings(question)])\n",
        "print('Question embedding shape: ', question_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ZVV41PgrxdM",
        "outputId": "e64362da-4630-4963-a6c3-f6b7b89bcca2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[419 420]]\n",
            "[[0.76932764 0.80099964]]\n"
          ]
        }
      ],
      "source": [
        "D, I = index.search(question_embeddings, k=1)\n",
        "print(I)\n",
        "print(D)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z1TyRlq5stcD"
      },
      "outputs": [],
      "source": [
        "retrieved_chunk = [crawl_content[i] for i in I.tolist()[0]]\n",
        "print(len(retrieved_chunk), retrieved_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ6g2ZdrrxdN",
        "outputId": "cfc003a8-1395-4b84-ab17-96d8384c0e32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Context information is below.\n",
            "---------------------\n",
            "['\\n                    Title: Boosting RAG: Picking the Best Embedding & Reranker models\\n                    -----------\\n                    Session title: Impact of Rerankers:\\n                    -----------\\n                    Content: \\nWithoutReranker: This provides the baseline performance for each embedding.\\nbge-reranker-base: Generally improves both hit rate and MRR across embeddings.\\nbge-reranker-large: This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the CohereRerank.\\nCohereRerank: Consistently enhances performance across all embeddings, often providing the best or near-best results.    \\n                    ']\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\n",
            "Answer:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{retrieved_chunk}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwpzIZJArxdN",
        "outputId": "d8e59a82-8391-4b40-ed23-402c3f8cccaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The two main metrics used to evaluate the performance of the different rerankers in the RAG system are hit rate and MRR (Mean Reciprocal Rank).\n",
            "Time:  1.470202922821045\n"
          ]
        }
      ],
      "source": [
        "def run_llm(user_message, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [\n",
        "        dict(role=\"user\", content=user_message)\n",
        "    ]\n",
        "    chat_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)\n",
        "\n",
        "from time import time\n",
        "\n",
        "ts = time()\n",
        "answer = run_llm(prompt)\n",
        "print(answer)\n",
        "\n",
        "print('Time: ', time() - ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sK_qcECBrxc7"
      },
      "source": [
        "### 2. Building RAG System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRnSPng9rxc8"
      },
      "outputs": [],
      "source": [
        "# Load the crawl dict from the file\n",
        "with open('crawl_content.pkl', 'rb') as crawl_file:\n",
        "    crawl_content = pickle.load(crawl_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JHa4snOrxc9"
      },
      "outputs": [],
      "source": [
        "with open('../API_key', 'r') as f:\n",
        "    os.environ['OPENAI_API_KEY'] = f.read()\n",
        "\n",
        "# Initialize an OpenAI instance\n",
        "client = OpenAI(api_key=os.environ['OPENAI_API_KEY'])\n",
        "\n",
        "def get_text_embeddings(input):\n",
        "    embeddings_batch_response = client.embeddings.create(\n",
        "        model='text-embedding-3-small',\n",
        "        input = input\n",
        "    )\n",
        "    return embeddings_batch_response.data[0].embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Whv1_XU-rxc9",
        "outputId": "bf29f16d-7c5e-4cb8-97f3-be6d8ee7adae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of text embedding:  (649, 1536)\n"
          ]
        }
      ],
      "source": [
        "# Load embeddings from file\n",
        "with open('embeddings.pkl','rb') as f:\n",
        "    text_embeddings = pickle.load(f)\n",
        "\n",
        "print('Shape of text embedding: ',text_embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FlRgIvcrxc_"
      },
      "outputs": [],
      "source": [
        "d = text_embeddings.shape[1]\n",
        "index = faiss.IndexFlatL2(d)\n",
        "index.add(text_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCR7ZPG1rxdB",
        "outputId": "c96e3fc6-8f8f-41d5-8dd4-c904ab1d869e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question embedding shape:  (1, 1536)\n",
            "[[419]]\n",
            "[[0.76145077]]\n",
            "1 ['\\n                    Title: Boosting RAG: Picking the Best Embedding & Reranker models\\n                    -----------\\n                    Session title: Impact of Rerankers:\\n                    -----------\\n                    Content: \\nWithoutReranker: This provides the baseline performance for each embedding.\\nbge-reranker-base: Generally improves both hit rate and MRR across embeddings.\\nbge-reranker-large: This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the CohereRerank.\\nCohereRerank: Consistently enhances performance across all embeddings, often providing the best or near-best results.    \\n                    ']\n"
          ]
        }
      ],
      "source": [
        "question = 'What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system? '\n",
        "question_embeddings = np.array([get_text_embeddings(question)])\n",
        "print('Question embedding shape: ', question_embeddings.shape)\n",
        "\n",
        "D, I = index.search(question_embeddings, k=1)\n",
        "print(I)\n",
        "print(D)\n",
        "\n",
        "retrieved_chunk = [crawl_content[i] for i in I.tolist()[0]]\n",
        "print(len(retrieved_chunk), retrieved_chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BZkjSCwrxdC",
        "outputId": "db482e57-f950-437e-9098-95b39ca39227"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Context information is below.\n",
            "---------------------\n",
            "['\\n                    Title: Boosting RAG: Picking the Best Embedding & Reranker models\\n                    -----------\\n                    Session title: Impact of Rerankers:\\n                    -----------\\n                    Content: \\nWithoutReranker: This provides the baseline performance for each embedding.\\nbge-reranker-base: Generally improves both hit rate and MRR across embeddings.\\nbge-reranker-large: This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the CohereRerank.\\nCohereRerank: Consistently enhances performance across all embeddings, often providing the best or near-best results.    \\n                    ']\n",
            "---------------------\n",
            "Given the context information and not prior knowledge, answer the query.\n",
            "Query: What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system? \n",
            "Answer:\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prompt = f\"\"\"\n",
        "Context information is below.\n",
        "---------------------\n",
        "{retrieved_chunk}\n",
        "---------------------\n",
        "Given the context information and not prior knowledge, answer the query.\n",
        "Query: {question}\n",
        "Answer:\n",
        "\"\"\"\n",
        "\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypncK6SBrxdD",
        "outputId": "1919192d-fa39-44a4-bb4e-f6fea9993c12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The two main metrics used to evaluate the performance of the different rerankers in the RAG system are hit rate and Mean Reciprocal Rank (MRR).\n",
            "Time:  1.1446759700775146\n"
          ]
        }
      ],
      "source": [
        "def run_llm(user_message, model=\"gpt-3.5-turbo\"):\n",
        "    messages = [\n",
        "        dict(role=\"user\", content=user_message)\n",
        "    ]\n",
        "    chat_response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )\n",
        "    return (chat_response.choices[0].message.content)\n",
        "\n",
        "from time import time\n",
        "\n",
        "ts = time()\n",
        "answer = run_llm(prompt)\n",
        "print(answer)\n",
        "\n",
        "print('Time: ', time() - ts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry9H1oVmrxdO"
      },
      "source": [
        "## 3. Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2EWEb-SrxdP",
        "outputId": "2ba52081-f632-4407-f490-4c8647640eab"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\khanhthu\\AppData\\Roaming\\Python\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# Question/Context/Groundtruth\n",
        "from ragas.testset.generator import TestsetGenerator\n",
        "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
        "from llama_index.llms.openai import OpenAI\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Document\n",
        "\n",
        "# generator with openai models\n",
        "generator_llm = OpenAI(model=\"gpt-3.5-turbo-16k\")\n",
        "critic_llm = OpenAI(model=\"gpt-4\")\n",
        "embeddings = OpenAIEmbedding()\n",
        "\n",
        "generator = TestsetGenerator.from_llama_index(\n",
        "    generator_llm=generator_llm,\n",
        "    critic_llm=critic_llm,\n",
        "    embeddings=embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Te1CO3-prxdP",
        "outputId": "d09a7268-46c9-46f3-e9be-b8f30330a1a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'page_label': '1', 'file_name': 'attention.pdf', 'file_path': 'attention.pdf', 'file_type': 'application/pdf', 'file_size': 2215244, 'creation_date': '2024-07-21', 'last_modified_date': '2024-07-12'}\n"
          ]
        }
      ],
      "source": [
        "from llama_index.core import SimpleDirectoryReader\n",
        "documents = SimpleDirectoryReader(input_files=[\"attention.pdf\"]).load_data()\n",
        "print(documents[0].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6DkHiBFrxdQ",
        "outputId": "0215f352-13b3-4b50-a591-cb96a4ebf586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Doc ID: 2be60230-2622-4b6f-9353-452e5e55e456\n",
            "Text: Title: Case Study: How Scaleport.ai Accelerated Development and\n",
            "Improved Sales with LlamaCloud                     -----------\n",
            "Session title: The Challenge: Streamlining AI Development\n",
            "-----------                     Content:  Scaleport AI specializes in\n",
            "transforming emerging AI technology into tangible bu...\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "with open('crawl_content.pkl', 'rb') as f:\n",
        "    crawl_content = pickle.load(f)\n",
        "documents = [Document(text=content) for content in crawl_content]\n",
        "print(documents[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llama-chatbot",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
