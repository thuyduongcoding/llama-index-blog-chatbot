{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings, Document, VectorStoreIndex, SummaryIndex, StorageContext, load_index_from_storage\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.llms.openai import OpenAI\n",
    "from openai import RateLimitError\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core.evaluation import FaithfulnessEvaluator, RelevancyEvaluator, RetrieverEvaluator, DatasetGenerator, BatchEvalRunner\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import time\n",
    "\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building RAG System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the embed model.\n",
      "---\n",
      "Already created indexes\n",
      "Loaded indexes from storage\n",
      "---\n",
      "Created the query engine\n",
      "---\n",
      "Created query tool\n",
      "---\n",
      "Created the router query engine.\n",
      "---\n",
      "Created the retriever\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "def retry_with_exponential_backoff(\n",
    "    func,\n",
    "    initial_delay: float = 1,\n",
    "    exponential_base: float = 2,\n",
    "    jitter: bool = True,\n",
    "    max_retries: int = 15,\n",
    "    errors: tuple = (RateLimitError,),\n",
    "):\n",
    "    \"\"\"Retry a function with exponential backoff.\"\"\"\n",
    "\n",
    "    def wrapper(*args, **kwargs):\n",
    "        # Initialize variables\n",
    "        num_retries = 0\n",
    "        delay = initial_delay\n",
    "\n",
    "        # Loop until a successful response or max_retries is hit or an exception is raised\n",
    "        while True:\n",
    "            try:\n",
    "                return func(*args, **kwargs)\n",
    "\n",
    "            # Retry on specific errors\n",
    "            except errors as e:\n",
    "                # Increment retries\n",
    "                num_retries += 1\n",
    "                print('Number of retries: ',num_retries)\n",
    "\n",
    "                # Check if max retries has been reached\n",
    "                if num_retries > max_retries:\n",
    "                    raise Exception(\n",
    "                        f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
    "                    )\n",
    "\n",
    "                # Increment the delay\n",
    "                delay *= exponential_base * (1 + jitter * random.random())\n",
    "\n",
    "                # Sleep for the delay\n",
    "                time.sleep(delay)\n",
    "\n",
    "            # Raise exceptions for any errors not specified\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "def query_chatbot(query_engine, query):\n",
    "    query_engine.query(query)\n",
    "\n",
    "@retry_with_exponential_backoff\n",
    "def query_with_backoff(**kwargs):\n",
    "    return query_chatbot(**kwargs)\n",
    "\n",
    "def query_from_llama_chatbot(query, query_engine,retriever):\n",
    "    # Query \n",
    "    # response = query_with_backoff(query_engine=query_engine, query=query)\n",
    "    response = query_engine.query(query)\n",
    "    # Retrieve relevant documents\n",
    "    nodes = retriever.retrieve(query)\n",
    "    \n",
    "    source = nodes[0].node.text\n",
    "    title = source.split('Title:')[1].split('\\n')[0].strip()\n",
    "    session_title = source.split('Session title:')[1].split('\\n')[0].strip()\n",
    "    content = source.split('Content:')[1].strip()\n",
    "\n",
    "    print('Answer: ', str(response))\n",
    "    print('-'*3)\n",
    "\n",
    "    print('Source used:')\n",
    "    print('- Blog Title: ', title)\n",
    "\n",
    "    if not (session_title == 'None'):\n",
    "        print('- Session title: ', session_title)\n",
    "\n",
    "    print('- Content: \\n', content)\n",
    "    print('-'*3)\n",
    "\n",
    "    return response, source\n",
    "    \n",
    "with open('../API_Key') as file:\n",
    "    os.environ['OPENAI_API_KEY'] = file.read() \n",
    "\n",
    "# Load the crawl dict from the file\n",
    "with open('crawl_content.pkl', 'rb') as crawl_file:\n",
    "    crawl_content = pickle.load(crawl_file)\n",
    "\n",
    "# Setting embedding and LLLM model\n",
    "Settings.llm = OpenAI(api_key=os.getenv('OPENAI_API_KEY'), model=\"gpt-4o-mini\")  \n",
    "Settings.embed_model = OpenAIEmbedding(api_key=os.getenv('OPENAI_API_KEY'), model=\"text-embedding-3-small\") \n",
    "print(\"Loaded the embed model.\")\n",
    "print('---')\n",
    "\n",
    "storage_context_name = 'storage_openai'\n",
    "\n",
    "if not (os.path.exists(f'./{storage_context_name}/vector_store') and os.path.exists(f'./{storage_context_name}/summary_store')):   \n",
    "    # Save data into Document object\n",
    "    documents = [Document(text=content) for content in tqdm(crawl_content)]\n",
    "    print(f\"Loaded {len(documents)} documents.\")\n",
    "    print('---')\n",
    "\n",
    "    # Create vector and summary store index from documents\n",
    "    print(\"There are no index storage yet\")\n",
    "    print(\"Start to create indexes...\")\n",
    "    vector_index = VectorStoreIndex.from_documents(documents,show_progress=True)\n",
    "    summary_index = SummaryIndex.from_documents(documents,show_progress=True)\n",
    "    print(\"Created the indexes.\")\n",
    "\n",
    "    # Store the vector and summary indexes\n",
    "    print(\"Start to store the indexes...\")\n",
    "    vector_index.storage_context.persist(persist_dir=f'./{storage_context_name}/vector_store')\n",
    "    summary_index.storage_context.persist(persist_dir=f'./{storage_context_name}/summary_store')\n",
    "    print('Store the indexes')\n",
    "    print('---')\n",
    "else:\n",
    "    print('Already created indexes')\n",
    "    # Load the vector and summary store index\n",
    "    vector_storage_context = StorageContext.from_defaults(persist_dir=f\"./{storage_context_name}/vector_store\")\n",
    "    vector_index = load_index_from_storage(vector_storage_context)\n",
    "\n",
    "    summary_storage_context = StorageContext.from_defaults(persist_dir=f\"./{storage_context_name}/summary_store\")\n",
    "    summary_index = load_index_from_storage(summary_storage_context)\n",
    "\n",
    "    print('Loaded indexes from storage')\n",
    "    print('---')\n",
    "\n",
    "# Create the query engine from vector store index\n",
    "vector_query_engine = vector_index.as_query_engine(similarity_top_k = 1)\n",
    "summary_query_engine = summary_index.as_query_engine(response_mode = \"tree_summarize\", use_async = True)\n",
    "print('Created the query engine')\n",
    "print('---')\n",
    "\n",
    "# Create tools for query engine\n",
    "summary_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=summary_query_engine, \n",
    "    description=\"Useful for summarization questions related to the llama-index blogs\")\n",
    "\n",
    "vector_tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=vector_query_engine,\n",
    "    description=\"Useful for retreiving specific information in the llama-index blogs\"\n",
    ")\n",
    "print(\"Created query tool\")\n",
    "print(\"-\"*3)\n",
    "\n",
    "\n",
    "query_engine = RouterQueryEngine(\n",
    "    selector=LLMSingleSelector.from_defaults(),\n",
    "    query_engine_tools=[\n",
    "        summary_tool,\n",
    "        vector_tool,\n",
    "    ],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Created the router query engine.\")\n",
    "print('-'*3)\n",
    "\n",
    "# Create the retriever\n",
    "retriever = vector_index.as_retriever(similarity_top_k = 2)\n",
    "print('Created the retriever')\n",
    "print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator\n",
    "faithfulness_eval = FaithfulnessEvaluator()\n",
    "relevancy_eval = RelevancyEvaluator()\n",
    "correctness_eval = CorrectnessEvaluator()\n",
    "\n",
    "runner = BatchEvalRunner(\n",
    "    {'faithfulness': faithfulness_eval,\n",
    "     'relevancy': relevancy_eval},\n",
    "     workers=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_chatbot(query):\n",
    "    print (\"Now querying...\")\n",
    "    # Query  ---------------------------------------------\n",
    "    print (f\"Querying: {query}\")\n",
    "    print('-'*3)\n",
    "    query_from_llama_chatbot(query=query,query_engine=query_engine, retriever= retriever)\n",
    "\n",
    "    print('Evaluation:')\n",
    "    eval_results = runner.evaluate_queries(vector_index.as_query_engine(), queries=[query])\n",
    "\n",
    "    print('Faithfulness:')\n",
    "    print('- Passing: ', eval_results[\"faithfulness\"][0].passing)\n",
    "    print('- Score: ', eval_results[\"faithfulness\"][0].score)\n",
    "    print('-'*3)\n",
    "\n",
    "    print('Relevancy')\n",
    "    print('- Passing: ', eval_results[\"relevancy\"][0].passing)\n",
    "    print('- Score: ', eval_results[\"faithfulness\"][0].score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now querying...\n",
      "Querying: What are key features of llama-agents?\n",
      "---\n",
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question asks for specific information about key features, which aligns with retrieving specific information..\n",
      "\u001b[0mAnswer:  Key features of llama-agents include a distributed service-oriented architecture where each agent operates as an independently running microservice, a communication system utilizing standardized API interfaces through a central control plane, and the ability to define both agentic and explicit orchestration flows. Additionally, it offers ease of deployment for launching, scaling, and monitoring agents independently, along with scalability and resource management through built-in observability tools to track system performance and individual agent services.\n",
      "---\n",
      "Source used:\n",
      "- Blog Title:  Introducing llama-agents: A Powerful Framework for Building Production Multi-Agent AI Systems\n",
      "- Session title:  Key Features of llama-agents\n",
      "- Content: \n",
      " Distributed Service Oriented Architecture: every agent in LlamaIndex can be its own independently running microservice, orchestrated by a fully customizable LLM-powered control plane that routes and distributes tasks.\n",
      "Communication via standardized API interfaces: interface between agents using a central control plane orchestrator. Pass messages between agents using a message queue.\n",
      "Define agentic and explicit orchestration flows: developers have the flexibility to directly define the sequence of interactions between agents, or leave it up to an “agentic orchestrator” that decides which agents are relevant to the task.\n",
      "Ease of deployment: launch, scale and monitor each agent and your control plane independently.\n",
      "Scalability and resource management: use our built-in observability tools to monitor the quality and performance of the system and each individual agent service\n",
      "Let's dive into how you can start using llama-agents to build your own multi-agent systems.\n",
      "---\n",
      "Evaluation:\n",
      "Faithfulness:\n",
      "- Passing:  True\n",
      "- Score:  1.0\n",
      "---\n",
      "Relevancy\n",
      "- Passing:  True\n",
      "- Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "question1 = \"What are key features of llama-agents?\"\n",
    "ask_chatbot(question1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now querying...\n",
      "Querying: What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\n",
      "---\n",
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question asks for specific information regarding the critical areas of RAG system performance, which aligns with retrieving specific information..\n",
      "\u001b[0mAnswer:  The two critical areas of RAG system performance assessed in the 'Evaluating RAG with LlamaIndex' section are the Retrieval System and Response Generation.\n",
      "---\n",
      "Source used:\n",
      "- Blog Title:  OpenAI Cookbook: Evaluating RAG systems\n",
      "- Content: \n",
      " We’re excited to unveil our OpenAI Cookbook, a guide to evaluating Retrieval-Augmented Generation (RAG) systems using LlamaIndex. We hope you’ll find it useful in enhancing the effectiveness of your RAG systems, and we’re thrilled to share it with you.\n",
      "The OpenAI Cookbook has three sections:\n",
      "Understanding Retrieval-Augmented Generation (RAG): provides a detailed overview of RAG systems, including the various stages involved in building the RAG system.\n",
      "Building RAG with LlamaIndex: Here, we dive into the practical aspects, demonstrating how to construct a RAG system using LlamaIndex, specifically applied to Paul Graham’s essay, utilizing the VectorStoreIndex.\n",
      "Evaluating RAG with LlamaIndex: The final section focuses on assessing the RAG system’s performance in two critical areas: the Retrieval System and Response Generation.\n",
      "We use our unique synthetic dataset generation method, generate_question_context_pairs to conduct thorough evaluations in these areas.\n",
      "Our goal with this cookbook is to provide the community with an essential resource for effectively evaluating and enhancing RAG systems developed using LlamaIndex.\n",
      "Join us in exploring the depths of RAG system evaluation and discover how to leverage the full potential of your RAG implementations with LlamaIndex.\n",
      "Keep building with LlamaIndex!🦙\n",
      "Keep building with LlamaIndex!🦙\n",
      "---\n",
      "Evaluation:\n",
      "Faithfulness:\n",
      "- Passing:  True\n",
      "- Score:  1.0\n",
      "---\n",
      "Relevancy\n",
      "- Passing:  True\n",
      "- Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "question2 = \"What are the two critical areas of RAG system performance that are assessed in the 'Evaluating RAG with LlamaIndex' section of the OpenAI Cookbook?\"\n",
    "ask_chatbot(question2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now querying...\n",
      "Querying: What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\n",
      "---\n",
      "\u001b[1;3;38;5;200mSelecting query engine 1: The question asks for specific information regarding metrics used to evaluate performance, which aligns with retrieving specific information..\n",
      "\u001b[0mAnswer:  The two main metrics used to evaluate the performance of the different rerankers in the RAG system are hit rate and MRR (Mean Reciprocal Rank).\n",
      "---\n",
      "Source used:\n",
      "- Blog Title:  Boosting RAG: Picking the Best Embedding & Reranker models\n",
      "- Session title:  Impact of Rerankers:\n",
      "- Content: \n",
      " WithoutReranker: This provides the baseline performance for each embedding.\n",
      "bge-reranker-base: Generally improves both hit rate and MRR across embeddings.\n",
      "bge-reranker-large: This reranker frequently offers the highest or near-highest MRR for embeddings. For several embeddings, its performance rivals or surpasses that of the CohereRerank.\n",
      "CohereRerank: Consistently enhances performance across all embeddings, often providing the best or near-best results.\n",
      "---\n",
      "Evaluation:\n",
      "Faithfulness:\n",
      "- Passing:  True\n",
      "- Score:  1.0\n",
      "---\n",
      "Relevancy\n",
      "- Passing:  True\n",
      "- Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "question3 = \"What are the two main metrics used to evaluate the performance of the different rerankers in the RAG system?\"\n",
    "ask_chatbot(question3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
